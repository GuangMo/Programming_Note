## Note taking for algorithms
## Author: Guang Mo 

###### 7 Steps toward designing an efficient algorithm ########
1. Define computational problem 
2. Abstract irrelevant detail 
3. Reduce to a problem you learn in "Algorithm" text book 
4. Else design using "algorithmic toolbox" 
5. Analyze algorithm's scalability 
6. Implement & evaluate performance 
7. Repeat (optimize, generalize ) 


################### Analysis of Algorithms #########################


###########
########### Sorting
###########

#### Insertion Sort ##########


## Asymptotic analysis ## 
- Big O (upper bound), -Pi (lower bound), and Theta (between) 

# Solving Recurrences ## 
1. Substitution method
{
   a. Guess the form of the solution 
   b. Verify by induction 
   c. Solve for constants 
}



2. Iterating the recurrence 

3. Recursion tree 

4. Master method 

############################ Brute Force algorithm ############
- not really an algorithm, it simple checks element by element 

########################### Divide and Conquer ##################
- simple idea by dividing the big problem into small problems, and conquer one at a time. --- combine as well 

########################### Binary Search Trees (BSTs) #################
- all nodes in the left subtree is less or equal to the value of current node 
- all nodes in the right subtree is greater or equal to the value of current node 

Balanced search trees: trees which maintains height of O(lgn)

########## Red-black trees 
- requires an extra one-bit color field in each node
# Properties: 
1. Every node is either red or black 
2. The root and leaves (NIL's) are black 
3. If a node is red, then its parent is black 
4. All simple paths from any node x to a descendant leaf have the same number of black nodes == Black-height(x)  -- counting the NIL as one height
# Theorem: 
n keys has height,  h <= 2lg(n+1) 
# Query operations: 
Search, Min, Max, Successor, and Predecessor all run in O(lg n) time on a red-black tree with n nodes


########## End of Red-black trees 

######################################## Dynamic Programming
# Overview: 
# repeatedly calculation, with some of previous results saved in a table, which 
# avoid duplicated calculations. 
# - an example of time-memory trade-off 

# 4 Steps of applying dynamic programming
1. characterize the structure of an optimal solution 
2. Recursively define the value of an optimal solution 
3. Compute the value of an optimal solution 
4. Construct an optimal solution from computed information 

Optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which may be solved independently. 

Two equivalent ways to implement a dynamic-programming approach: 

Approach #1: top-down with memoization
- saves results, and checks if subproblem has been solved
- usually refers as # "depth-first search" of the subproblem graph


Approach #2: bottom-up method 
- depends on some natural notion of the "size" of a subproblem

Subproblems can be represented in subproblem graph with G=(V,E) 
- number of subproblems is equal to the number of vertices in the subproblem graph

#15.2 Matrix-chain multiplication
- How we parenthesize a chain of matrics can have a dramatic impact on the cost of evaluating the product. 

# matrix-chain multiplication problem: given a chain of n matrics, fully parenthesize the prduct in a way that minimizes the number of scalar multiplications. 
 

#15.3 Elements of dynamic programming 
- Two key ingredients: optimal substructure and overlapping subproblems. 

Optimal substructure, common steps to identify optimal substructure
1. solution to the problem consists of making a choice, making such choice leaves one of more subproblems to be solved 

2. you are given the choice that leads to an optimal solution, and you assume that it has been given to you ONLY

3. Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems 

4. show that the solutions to the subproblems used within an optimal solution must themselves be optimal using "cut-and-paste" technique. 


Overlapping subproblems: 
-Total number of distinct subproblems is a polynomial in the input size

# running time of a dynamic-programming algorithm depends on the product of two facts: the number of subproblems overall and how many choices we look at for each subproblem. 

subproblems need to be independent, solving one problem does not affect the other problem. 
##### End of dynamic programming



###################################### Greedy Algorithms #####################
# Overview: picks local maximum at each entry, not neccessarily yeilds 
#	    absolute maximum. 
#####


###################################### Graphic Algorithm #####################
# Graphic is represented by vertexes and edges, and each edge is represented 
# with adjacent vertexes. A graph can be represented with either adjacency-list
# or adjacency-matrix representation. 
##############################################################################

##############################################################################
# Breadth-first serach algorithm
##############################################################################
Overview: "Given a graph G = (V,E) and a distinguished source vertex s, breadth-first search systematically explores the edges of G to “discover” every vertex that is reachable from s. It computes the distance (smallest number of edges) from s to each reachable vertex. It also produces a “breadth-first tree” with root s that contains all reachable vertices. For any vertex v reachable from s, the simple path in the breadth-first tree from s to v corresponds to a “shortest path” from s to v in G, that is, a path containing the smallest number of edges. The algorithm works on both directed and undirected graphs. " 

##############################################################################
# Depth-first search
##############################################################################
Overview: to search "deeper" whenever possible
The predecessor subgraph of a depth-first search forms a depth-first forest comprising several depth-first trees

Pseudocode: 
DFS(G)
for each vertex u in G.V
	u.volor = WHITE
	u.pi = NIL 
time = 0 
for each vertex u in G.V 
	if u.color == WHITE
		DFS-VISIT(G,u) 

DFS-VISIT(G,u): 
time = time + 1; 
u.d = time; 
u.color = GRAY;
for each v in G.Adj[u]
	if v.color == WHITE
		v.pi = u
		DFS-VISIT(G,v); 
u.color = BLACK 
time = time + 1
u.f = time 

Properties: 
1. a vertex v is a desendant of vertex u in the depth-first forest if and only if v is discovered during the time in which u is gray. 
2. discovery and finishing times have parenthesis structure( bounded by timestamps)

White-path theorem: 
vertex v is a descendant of vertex u if and only if at the time u.d that the search discovers u, there is a path from u to v consisting entirely of white vertices. 


################################################################################# Bit Manipulation 
################################################################################
When performing bit shifting, need to be cautious about boundry condition, eg, register can't hold the value. 

Xor: 0 ^ 0 = 0; 1 ^ 0 = 1; 0 ^ 1 = 1; 1 ^ 1 = 0; 

Left Shift: 
x << y means x shifted y bits to the left, out of boundary, bits just dropped off 
Right shift: 
x >> y 
 

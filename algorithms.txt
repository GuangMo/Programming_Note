## Note taking for algorithms
## Author: Guang Mo 

###### 7 Steps toward designing an efficient algorithm ########
1. Define computational problem 
2. Abstract irrelevant detail 
3. Reduce to a problem you learn in "Algorithm" text book 
4. Else design using "algorithmic toolbox" 
5. Analyze algorithm's scalability 
6. Implement & evaluate performance 
7. Repeat (optimize, generalize ) 


################### Analysis of Algorithms #########################


###########
########### Sorting
###########

############################# Insertion Sort ###########################


############################# Begin Heapsort ###########################
Runtime: O(nlgn), which combines the better attributes of merge sort and insertion sort. 

Heap: a data structure that is an array which is almost binary tree
Array A, two attributes.
A.length --> total length of an array 
A.heap-size --> actual number elements stored in the array 

PARENT(i): 
1. return [i/2] 

LEFT(i):
1   return 2i 

RIGHT(i): 
1   return 2i+1 

Two types of heap: max-heaps, and min-heap
 eg, max-heaps:   A[PARENT(i)] >= A[i] 
     min-heaps:   A[PARENT(i)] <= A[i] 

height is defined as: the number of edges on the longest simple downward path from the node to a leaf. 

MAX-HEAPIFY: maintaining the max-heap property, runs in O(lgn) time 
BUILD-MAX-HEAP: runs in linear time, produces a max heap from unorder input array
HEAPSORT: O(nlgn), sorts an array in place
MAX-HEAP-INSERT, HEAP-EXTRACT-MAX,HEAP-INCREASE-KEY, HEAP-MAXIMUM run in O(lgn) 

Building a heap: 
- arrange in an array

BUILD-MAX-HEAP(A):
1  A.heap-size = A.length
2  for i = [A.length/2] downto 1 
3     MAX-HEAPIFY(A,i)

# Sorting algorithm: Heapsort(A) 
- key idea is to extract one element at a time then resort the heap
HEAPSORT(A): 
1 BUILD-MAX-HEAP(A)
2 for i = A.length downto 2 
3    exchange A[1] with A[i] 
4    A.heap-size = A.heap-size - 1 
5    MAX-HEAPIFY(A,1) 

# Priority queues
- most of use of heap
- max-priority queues and min-priority queues, elements are associated with a key 

A max-priority queue has following operation: 
-INSERT( S, x) inserts the element x into the set S
-MAXIMUM(S) returns the element of S with the largest key 
-EXTRACT-MAX(S) removes and returns the element of S with the largest key 
-INCREASE-KEY(S,x,k) increases the value of element x's key to the new value
	k, which is assumed to be at least as large as x's current key value

HEAP-MAXIMUM(A)
1   return A[1] 

HEAP-EXTRACT-MAX(A)
1 if A.heap-size < 1 
2    error "heap underflow" 
3 max = A[1] 
4 A[1] = A[A.heap-size] 
5 A.heap-size = A.heap-size -1 
6 MAX-HEAPIFY(A,1) 
7 return max

HEAP-INCREASE-KEY(A,i,key):
1 if key < A[i]
2    error "new key is smaller than current key" 
3 A[i] = key
4 while i > 1 and A[PARENT(i) < A[i]
5    exchange A[i] with A[PARENT(i)] 
6    i = PARENT(i)

MAX-HEAP-INSERT(A,key): 
1 A.heap-size = A.heap-size + 1 
2 A[A.heap-size] = -INFINITE
3 HEAP-INCREASE-KEY(A,A.heap-size, key) 

===> The functions are highly dependend on one or other, over-all complexity must be analyze thoroughly. 

############################# End Heapsort ###########################

############################# Beginning of  Quicksort ##########################
Overview: worse case O(n^2), but it is often the best practical choice for
sorting
average: O(nlgn), with small constant factor, sorting in place
- an application of three-step divide-and-conquer process

# Divide: Partition the array A[p..r] into two possibly empty subarray A[p..q-1] and A[q+1..r], A[p] is the cut off element

# Conquer: Sort the two subarrays A[p..q-1] and A[q+1..r] by recursive calls to quicksort

#Combine: Because the subarrays are already sorted, no work is needed to combine them; the entire array A[p..r] is now sorted 

QUICKSORT(A,p,r)
1 if p < r
2    q = PARTITION(A,p,r) 
3    QUICKSORT(A,p,q-1)
4    QUICKSORT(A,q+1,r) 

Partitioning the array
- purpose: It is to divide an array, such that the array on the left is 
smaller or equal to the index, and the array on the right is large or equal
to the index. 

PARTITION(A,p,r):
1 x=A[r]
2 i = p - 1; 
3 for j = p to r - 1 
4    if A[j] <= x
5 	 i = i + 1   # moving one of the list 
6	 exchange A[i] with A[j]  # the pivot is only changed when it is smaller
7 exchange A[i+1] with A[r]    
8 return i + 1; 

#Performance of quicksort: 
- almost always the same for average case 

# Random sampling
- randomly chosen element instead of using A[r] 
RANDOMIZED-PARTITION(A,p,r): 
1 i = RANDOM(p,r)
2 exchange A[r] with A[i] 
3 return PARTITION(A,p,r) 

############################# End Quicksort ###########################

############################# Sorting in Linear Time###########################
Comparison Sorts: sorted order based on comparisons between the input elements

Counting sort, radix sort, and bucket sort run in linear time 

# Lower bounds for sorting: 
The decision-tree model: each internal node by i:j , represents a comparison A[i] <= A[j]

# Counting sort 
- assumes input elements are in range of 0-k 
- 3 arrays needed, input A, output B, and C for temporary working storage C[0..k]

COUNTING-SORT(A,B,k):
1 let C[0..k] be a new array
2 for i = 0 to k 
3    C[i] = 0 
4 for j = 1 to A.length 
5    C[A[j]] = C[A[j]]+1
6 //C[i] now contains the number of elements equal to i
7 for i = 1 to k
8    C[i] = C[i] + C[i-1]
9 //C[i] now contains the number of elements less than or equal to i
10 for j = A.length downto 1 
11    B[C[A[j]]] = A[j]
12    C[A[j]] = C[A[j]] - 1 

values are basically exchanged via array C
O(n) when the assumption is correct

# Radix sort


############################# End Linear time sorting #########################
## Asymptotic analysis ## 
- Big O (upper bound), -Pi (lower bound), and Theta (between) 

# Solving Recurrences ## 
1. Substitution method
{
   a. Guess the form of the solution 
   b. Verify by induction 
   c. Solve for constants 
}



2. Iterating the recurrence 

3. Recursion tree 

4. Master method 

############################ Brute Force algorithm ############
- not really an algorithm, it simple checks element by element 

########################### Divide and Conquer ##################
- simple idea by dividing the big problem into small problems, and conquer one at a time. --- combine as well 

########################### Binary Search Trees (BSTs) #################
- all nodes in the left subtree is less or equal to the value of current node 
- all nodes in the right subtree is greater or equal to the value of current node 

Balanced search trees: trees which maintains height of O(lgn)

########## Red-black trees 
- requires an extra one-bit color field in each node
# Properties: 
1. Every node is either red or black 
2. The root and leaves (NIL's) are black 
3. If a node is red, then its parent is black 
4. All simple paths from any node x to a descendant leaf have the same number of black nodes == Black-height(x)  -- counting the NIL as one height
# Theorem: 
n keys has height,  h <= 2lg(n+1) 
# Query operations: 
Search, Min, Max, Successor, and Predecessor all run in O(lg n) time on a red-black tree with n nodes


########## End of Red-black trees 

######################################## Dynamic Programming
# Overview: 
# repeatedly calculation, with some of previous results saved in a table, which 
# avoid duplicated calculations. 
# - an example of time-memory trade-off 

# 4 Steps of applying dynamic programming
1. characterize the structure of an optimal solution 
2. Recursively define the value of an optimal solution 
3. Compute the value of an optimal solution 
4. Construct an optimal solution from computed information 

Optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which may be solved independently. 

Two equivalent ways to implement a dynamic-programming approach: 

Approach #1: top-down with memoization
- saves results, and checks if subproblem has been solved
- usually refers as # "depth-first search" of the subproblem graph


Approach #2: bottom-up method 
- depends on some natural notion of the "size" of a subproblem

Subproblems can be represented in subproblem graph with G=(V,E) 
- number of subproblems is equal to the number of vertices in the subproblem graph

#15.2 Matrix-chain multiplication
- How we parenthesize a chain of matrics can have a dramatic impact on the cost of evaluating the product. 

# matrix-chain multiplication problem: given a chain of n matrics, fully parenthesize the prduct in a way that minimizes the number of scalar multiplications. 
 

#15.3 Elements of dynamic programming 
- Two key ingredients: optimal substructure and overlapping subproblems. 

Optimal substructure, common steps to identify optimal substructure
1. solution to the problem consists of making a choice, making such choice leaves one of more subproblems to be solved 

2. you are given the choice that leads to an optimal solution, and you assume that it has been given to you ONLY

3. Given this choice, you determine which subproblems ensue and how to best characterize the resulting space of subproblems 

4. show that the solutions to the subproblems used within an optimal solution must themselves be optimal using "cut-and-paste" technique. 


Overlapping subproblems: 
-Total number of distinct subproblems is a polynomial in the input size

# running time of a dynamic-programming algorithm depends on the product of two facts: the number of subproblems overall and how many choices we look at for each subproblem. 

subproblems need to be independent, solving one problem does not affect the other problem. 
##### End of dynamic programming



###################################### Greedy Algorithms #####################
# Overview: picks local maximum at each entry, not neccessarily yeilds 
#	    absolute maximum. 
#####


###################################### Graphic Algorithm #####################
# Graphic is represented by vertexes and edges, and each edge is represented 
# with adjacent vertexes. A graph can be represented with either adjacency-list
# or adjacency-matrix representation. 
##############################################################################

##############################################################################
# Breadth-first serach algorithm
##############################################################################
Overview: "Given a graph G = (V,E) and a distinguished source vertex s, breadth-first search systematically explores the edges of G to “discover” every vertex that is reachable from s. It computes the distance (smallest number of edges) from s to each reachable vertex. It also produces a “breadth-first tree” with root s that contains all reachable vertices. For any vertex v reachable from s, the simple path in the breadth-first tree from s to v corresponds to a “shortest path” from s to v in G, that is, a path containing the smallest number of edges. The algorithm works on both directed and undirected graphs. " 

##############################################################################
# Depth-first search
##############################################################################
Overview: to search "deeper" whenever possible
The predecessor subgraph of a depth-first search forms a depth-first forest comprising several depth-first trees

Pseudocode: 
DFS(G)
for each vertex u in G.V
	u.volor = WHITE
	u.pi = NIL 
time = 0 
for each vertex u in G.V 
	if u.color == WHITE
		DFS-VISIT(G,u) 

DFS-VISIT(G,u): 
time = time + 1; 
u.d = time; 
u.color = GRAY;
for each v in G.Adj[u]
	if v.color == WHITE
		v.pi = u
		DFS-VISIT(G,v); 
u.color = BLACK 
time = time + 1
u.f = time 

Properties: 
1. a vertex v is a desendant of vertex u in the depth-first forest if and only if v is discovered during the time in which u is gray. 
2. discovery and finishing times have parenthesis structure( bounded by timestamps)

White-path theorem: 
vertex v is a descendant of vertex u if and only if at the time u.d that the search discovers u, there is a path from u to v consisting entirely of white vertices. 

#################################################################################
# Minimum-spanning-tree 
################################################################################# 
- connected, undirected graph and it is acyclic, which forms a tree (no cycle) 

Solving for minimum-spanning-tree problem


### Kruskal's algorithm 
The set A is a forest whose vertices are all those of the given graph, The safe edge added to A is always a least-weight edge in the graph that connects two distinct components. 

MST-KRUSKAL(G,w):
1  A = 0; 
2  for each vertex v in G.V: 
3	MAKE-SET(v) 
4  sort the edges of G.E into nondecreasing order by weight w 
5  for each edge (u,v) in G.E, taken in nondecreasing order by weight
6	if FIND-SET(u) != FIND-SET(v)
7		A = A OR {(u,v)}
8		UNION(u,v)
9  return A

Total running time is O(ElgE) 

### Prim's algorithm 
The set A forms a single tree. The safe edge added to A is always a least-weight edge connecting the tree to a vertex not in the tree. 

- need a fast way to select a new edge to add to the tree formed by the edges in A. 

MST-PRIM(G,w,r): 
1 for each u in G.V 
2    u.key = INFINITE 
3    u.pi = NIL 
4 r.key = 0 
5 Q = G.V 
6 while Q != 0 
7    u = EXTRACT-MIN(Q); 
8    for each v in G.Adj[u]:   # this loop updates the info only, but not added to tree 
9	if v in Q and w(u,v) < v.key
10	   v.pi = u   ## becomes parent 
11	   v.key = w(u,v)


###############################################################################
# Single-Source Shortest Paths 
###############################################################################
- weighted, directed graph G=(V,E)
If there is a negative-weight cycle on some path from s to v, we define &(s,v) = -INFINITY 

- as long as a shortest path has 0-weight cycles, we can repeatedly remove 
these cycles from the path until we have a shortest path that is cycle-free

acyclic path contains at most |V| - 1 edges

A shortest-paths tree rooted at s is a directed subgraph G' = (V', E')
1. V' is the set of vertices reachable from s in G 
2. G' forms a rooted tree with root s, and 
3. for all v in V', the unique simple path from s to v in G' is a shortest
path from s to v in G. 

# Relaxation technique 
 maintain an attribute v.d, which is an upper bound on the weight of a shortest path from source s to v. v.d is shortest-path estimate

INITIALIZE-SINGLE-SOURCE(G,s): 
1   for each vertex v in G.V
2	v.d = INFINITY
3	v.pi = NIL 
4.  s.d = 0 

RELAX(u,v,w): 
1   if v.d > u.d + w(u,v)
2	v.d = u.d + w(u,v) 
3	v.pi = u

#### Properties of shortest paths and relaxation: 
Triangle inequality: 
For any edge (u,v) in E, we have Shortest(s,v) <= Shortest(s,u) + w(u,v)

Upper-bound property: 
We always have v.d >= Shortest(s,v) for all vertices v in V, and once v.d achieves the value Shortest(s,v), it never changes 

No-path property: 
If there is no path from s to v, then we always have v.d = Shortest(s,v) =
INFINITY 

Convergence property: 
If s->u -> v is a shortest path in G for some u,v in V, and if u.d = Shortest(s,u) at any time prior to relaxing edge (u,v), then v.d = Shortest(s,v) at all times afterward. 

Path-relaxation property: 
If p is a shortest path, and relaxation estimate is each Shortest(s, vk)

Predecessor-subgraph property: 
Once v.d = Shortest(s,v) for all v in V, the predecessor subgraph is a 
shortest-paths tree rooted at s. 

#############################################################################  The Bellman-Ford algorithm
############################################################################
The Bellman-Ford algorithm solves the single-source shortest-paths problem in the general case in which edge weights may be negative. Given a weighted, directed graph G= (V,E) with source s and weight function w:E -> R, the Bellman-Ford algorithm returns a boolean value indicating whether or not there is a negative-weight cycle that is reachable from the source. If there is such a cycle, the algorithm indicates that no solution exists. If there is no such cycle, the algorithm produces the shortest paths and their weights. 

BELLMAN-FORD(G,w,s): 
1 INITIALIZE-SINGLE-SOURCE(G,s)
2 for i = 1 to |G.V| - 1 
3    for each edge (u,v) in G.E 
4	RELAX(u,v,w)
5 for each edge (u,v) in G.E
6    if v.d > u.d + w(u,v) 
7	 return FALSE
8 return TRUE 

############################################################################# Dijkstra's algorithm 
############################################################################
DIJKSTRA(G,w,s): 
1 INITIALIZE-SINGLE-SOURCE(G,s)
2 S = 0
3 Q = G.V
4 while Q != 0 
5    u = EXTRACT-MIN(Q)
6    S = S union {U}
7    for each vertex v in G.Adj[u] 
8	RELAX(u,v,w)

#Correctness of Dijkstra's algorithm 
Dijkstra's algorithm, run on a weighted, directed graph G = (V,E) with
non-negative weight function w and source s, terminates with u.d = Shortest(s,u) for all vertices u belongs V

########################################################################
### Difference constraints and shortest paths 
########################################################################
Feasibility problem through linear programming 

# Systems of difference constraints 

Let x = (x1,x2,...,xn) be a solution to a system Ax <= b of difference constraints, and let d be any constant. Then x + d = (x1+d, x2+d,...,xn + d) is a solution to Ax <= b as well 

########################################################################
### All-Pairs Shortest Paths 
########################################################################


########################################################################
### Maximum Flow 
########################################################################
Flow networks: 
A flow network G = (V,E) is a directed graph in which each edge (u,v) in 
E has a nonnegative capacity c(u,v) >= 0. 
Since each vertex other than s has at least one entering edge, |E| >= |V| 

########################################################################
### The Ford-Fulkerson method 
########################################################################
- depends on 3 important ideas 
	1. residual networks
	2. augmenting paths
	3. cuts

FORD-FULKERSON-METHOD(G,s,t): 
1 initialize flow f to 0 
2 while there exists an augmenting path p in the residual network Gf 
3    augment flow f along p 
4 return f 

# Residual networks: 
The only edges of G that are in Gf are those that can admit more flow

residual capacity cf(u,v) = v(u,v) -f(u,v) 

cf(p) = min{cf(u,v):(u,v) is on p} 

A flow is maximum if and only if its residual network contains no 
augmenting path

################################################################################# Bit Manipulation 
################################################################################
When performing bit shifting, need to be cautious about boundry condition, eg, register can't hold the value. 

Xor: 0 ^ 0 = 0; 1 ^ 0 = 1; 0 ^ 1 = 1; 1 ^ 1 = 0; 

Left Shift: 
x << y means x shifted y bits to the left, out of boundary, bits just dropped off 
Right shift: 
x >> y 
 
